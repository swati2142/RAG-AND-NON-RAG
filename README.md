# ğŸ“š RAG vs Non-RAG Question Answering System

This project demonstrates the difference between **Retrieval-Augmented Generation (RAG)** and **Non-RAG (pure LLM)** question answering using a machine learning book as the knowledge source.

It allows users to:

âœ… Ask questions using **book context (RAG)**  
âœ… Ask questions using **model knowledge only (Non-RAG)**  
âœ… Compare accuracy and response quality  

---

## ğŸš€ Features

- ğŸ“„ Extracts text from PDF safely (handles corrupted pages)
- âœ‚ï¸ Splits text into chunks for efficient retrieval
- ğŸ” Creates semantic embeddings using Sentence Transformers
- âš¡ Stores embeddings using FAISS for fast similarity search
- ğŸ¤– Uses FLAN-T5 for answer generation
- ğŸ” Interactive CLI with RAG & Non-RAG modes

---

## ğŸ§  What is RAG?

**Retrieval-Augmented Generation (RAG)** improves answer accuracy by retrieving relevant context before generating an answer.

### âœ” RAG Workflow
1. User asks a question  
2. System retrieves relevant text from the PDF  
3. Context is provided to the model  
4. Model generates answer using that context  

### âœ” Non-RAG Workflow
- Model answers using **only pre-trained knowledge**

---

## ğŸ“‚ Project Structure


.
â”œâ”€â”€ RAG_AND_NON_RAG.ipynb
â”œâ”€â”€ 2019BurkovTheHundred-pageMachineLearning.pdf
â””â”€â”€ README.md


---

## âš™ï¸ Installation

Install required dependencies:

```bash
pip install pypdf sentence-transformers faiss-cpu transformers torch pymupdf
â–¶ï¸ How to Run
1ï¸âƒ£ Place the PDF in the project folder
2019BurkovTheHundred-pageMachineLearning.pdf
2ï¸âƒ£ Run the notebook

Open and execute:

RAG_AND_NON_RAG.ipynb
3ï¸âƒ£ Choose mode
1 â†’ RAG (uses book context)
2 â†’ Non-RAG (model knowledge)
4ï¸âƒ£ Ask questions interactively
ğŸ’¡ Example Questions
ğŸ“˜ RAG Mode

What is supervised learning?

Explain overfitting.

What is gradient descent?

ğŸ¤– Non-RAG Mode

What is machine learning?

Explain neural networks.

What is bias vs variance?

ğŸ” How It Works
Step 1: Load PDF

Extracts text safely

Skips corrupted pages

Step 2: Chunk Text

Splits text into small segments for retrieval.

Step 3: Create Embeddings

Model used:

all-MiniLM-L6-v2
Step 4: FAISS Index

Stores vectors for fast semantic search.

Step 5: Generator Model

Model used:

google/flan-t5-base
ğŸ“Š RAG vs Non-RAG Comparison
Feature	RAG	Non-RAG
Uses document context	âœ…	âŒ
More factual accuracy	âœ…	âŒ
Hallucination risk	Low	Higher
Domain-specific answers	Excellent	Limited
Requires embeddings	âœ…	âŒ
ğŸ¯ Use Cases

âœ… Study assistant for textbooks
âœ… Research document Q&A
âœ… Enterprise knowledge retrieval
âœ… Legal & policy document analysis
âœ… Academic learning tools

âš ï¸ Common Issues & Fixes
âŒ PDF loads but no text extracted

âœ” Try another PDF
âœ” Ensure it is not a scanned image

âŒ FAISS installation errors
pip install faiss-cpu
âŒ Slow first run

Models download during first execution.

ğŸ”® Future Improvements

Add web UI (Streamlit / Gradio)

Support multiple documents

Add source citations in answers

Use larger LLMs for improved explanations

Add chat history & memory

ğŸ‘¨â€ğŸ’» Author

Built to understand RAG vs LLM knowledge differences and modern AI retrieval systems.
